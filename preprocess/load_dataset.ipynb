{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-02-09 18:07:48,660 INFO] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from stog.utils.params import Params\n",
    "from stog.data.dataset_builder import dataset_from_params, iterator_from_params\n",
    "from stog.data.vocabulary import Vocabulary\n",
    "from stog.training.trainer import Trainer\n",
    "from stog.data.dataset import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adylanrff/Documents/Kuliah/TA/amr_parser/stog/utils/params.py:104: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  dict_merge.dict_merge(params_dict, yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "params = Params.from_file(\"../model/model_params.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-02-09 18:07:48,746 INFO] Building train datasets ...\n",
      "[2020-02-09 18:07:48,747 ERROR] Model name 'data/bert-base-cased/bert-base-cased-vocab.txt' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'data/bert-base-cased/bert-base-cased-vocab.txt' was a path or url but couldn't find any file associated to this path or url.\n",
      "0it [00:00, ?it/s][2020-02-09 18:07:48,750 INFO] Reading instances from lines in file at: ../data/raw/amr.txt.features\n",
      "[2020-02-09 18:07:48,825 INFO] POS tag coverage: 0.3087 (184/596)\n",
      "40it [00:00, 526.23it/s]\n",
      "[2020-02-09 18:07:48,826 INFO] Building dev datasets ...\n",
      "[2020-02-09 18:07:48,828 ERROR] Model name 'data/bert-base-cased/bert-base-cased-vocab.txt' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'data/bert-base-cased/bert-base-cased-vocab.txt' was a path or url but couldn't find any file associated to this path or url.\n",
      "0it [00:00, ?it/s][2020-02-09 18:07:48,829 INFO] Reading instances from lines in file at: ../data/raw/amr.txt.features\n",
      "23it [00:00, 163.12it/s][2020-02-09 18:07:49,002 INFO] POS tag coverage: 0.3087 (184/596)\n",
      "40it [00:00, 230.09it/s]\n",
      "[2020-02-09 18:07:49,004 INFO] Building test datasets ...\n",
      "[2020-02-09 18:07:49,005 ERROR] Model name 'data/bert-base-cased/bert-base-cased-vocab.txt' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese). We assumed 'data/bert-base-cased/bert-base-cased-vocab.txt' was a path or url but couldn't find any file associated to this path or url.\n",
      "0it [00:00, ?it/s][2020-02-09 18:07:49,007 INFO] Reading instances from lines in file at: ../data/raw/amr.txt.features\n",
      "[2020-02-09 18:07:49,073 INFO] POS tag coverage: 0.3087 (184/596)\n",
      "40it [00:00, 600.15it/s]\n"
     ]
    }
   ],
   "source": [
    "data_params = params['data']\n",
    "dataset = dataset_from_params(data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Batch' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-44448df74983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Batch' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train']\n",
    "dev_data = dataset.get('dev')\n",
    "test_data = dataset.get('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2020-02-09 19:06:23,823 INFO] Fitting token dictionary from dataset.\n",
      "100%|██████████| 40/40 [00:00<00:00, 2299.16it/s]\n",
      "[2020-02-09 19:06:23,850 WARNING] vocabulary serialization directory ../data/processed/serialization is not empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 20 31 74 75  3 76 77 32  3 78 79 80  5 81 82  2  0  0  0  0  0  0  0\n",
      "  0]\n",
      "[ 3 65  5  2 66 67  5 68 69 70 71 72 73  4  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0]\n",
      "{'src_tokens': {'num_tokens': 25, 'num_token_characters': 14}, 'src_must_copy_tags': {'num_tokens': 25}, 'tgt_tokens': {'num_tokens': 29, 'num_token_characters': 17}, 'src_pos_tags': {'num_tokens': 25}, 'tgt_pos_tags': {'num_tokens': 29}, 'tgt_copy_indices': {'num_tokens': 29}, 'tgt_copy_mask': {'num_tokens': 29}, 'tgt_copy_map': {'num_tokens': 29}, 'src_copy_indices': {'num_tokens': 29}, 'src_copy_map': {'num_tokens': 27}, 'head_tags': {'num_tokens': 29}, 'head_indices': {'num_tokens': 29}}\n"
     ]
    }
   ],
   "source": [
    "vocab_params = params.get('vocab', {})\n",
    "vocab = Vocabulary.from_instances(instances=train_data, **vocab_params)\n",
    "vocab.save_to_files(\"../data/processed/serialization\")\n",
    "\n",
    "dataset = Batch(train_data)\n",
    "dataset.index_instances(vocab)\n",
    "print(dataset.as_tensor_dict()['src_tokens']['encoder_tokens'][0].numpy())\n",
    "print(dataset.as_tensor_dict()['tgt_tokens']['decoder_tokens'][0].numpy())\n",
    "print(dataset.get_padding_lengths())\n",
    "# print(dataset.as_tensor_dict())\n",
    "\n",
    "train_iterator, dev_iterater, test_iterater = iterator_from_params(vocab, data_params['iterator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stog.data.iterators.bucket_iterator.BucketIterator at 0x7fb47589ea90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
